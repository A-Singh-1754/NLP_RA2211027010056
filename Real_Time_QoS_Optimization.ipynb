{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23b4322",
      "metadata": {
        "id": "e23b4322"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d53248",
      "metadata": {
        "id": "72d53248"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "def preprocess_qos_data(data):\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "    return data_scaled, scaler\n",
        "\n",
        "def preprocess_chat_data(chats, tokenizer, max_length=128):\n",
        "    inputs = tokenizer(chats, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a92d10",
      "metadata": {
        "id": "a6a92d10"
      },
      "outputs": [],
      "source": [
        "# DNN for QoS Satisfaction Prediction\n",
        "\n",
        "class QoSClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(QoSClassifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 2),  # Two classes: Satisfied, Unsatisfied\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097b8f80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "097b8f80",
        "outputId": "532d2c3c-ee20-4e85-8483-678526a8970a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# BERT-based Sentiment Analysis\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "def sentiment_analysis(chats):\n",
        "    inputs = preprocess_chat_data(chats, tokenizer)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6722c5",
      "metadata": {
        "id": "7a6722c5"
      },
      "outputs": [],
      "source": [
        "# Dynamic QoS Management\n",
        "\n",
        "def adjust_qos(predictions, sentiment):\n",
        "    if predictions == 1 or sentiment == 1:\n",
        "        print(\"Increasing bandwidth and reducing latency.\")\n",
        "    else:\n",
        "        print(\"Stable QoS parameters.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce614dab",
      "metadata": {
        "id": "ce614dab"
      },
      "outputs": [],
      "source": [
        "# Main Loop\n",
        "\n",
        "def main_loop(qos_data, chat_data):\n",
        "    qos_data, scaler = preprocess_qos_data(qos_data)\n",
        "    qos_model = QoSClassifier(input_dim=qos_data.shape[1])\n",
        "\n",
        "    for i, (metrics, chats) in enumerate(zip(qos_data, chat_data)):\n",
        "        metrics = torch.tensor(metrics, dtype=torch.float32).unsqueeze(0)\n",
        "        qos_prediction = qos_model(metrics)\n",
        "        chat_sentiment = sentiment_analysis([chats])\n",
        "\n",
        "        adjust_qos(qos_prediction.argmax().item(), chat_sentiment.item())\n",
        "        print(f\"Iteration {i+1}: QoS Prediction: {qos_prediction}, Chat Sentiment: {chat_sentiment}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33452d62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33452d62",
        "outputId": "08c16f3d-b08f-40bc-f920-6fc2cb7cd5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stable QoS parameters.\n",
            "Iteration 1: QoS Prediction: tensor([[0.5905, 0.4095]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 2: QoS Prediction: tensor([[0.5457, 0.4543]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 3: QoS Prediction: tensor([[0.5376, 0.4624]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 4: QoS Prediction: tensor([[0.5284, 0.4716]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example usage:\n",
        "# Sample DataFrame for QoS metrics\n",
        "qos_data = pd.DataFrame({\n",
        "    'latency': [20, 30, 25, 40],\n",
        "    'bitrate': [5000, 4500, 5200, 4800],\n",
        "    'packet_loss': [0.1, 0.05, 0.07, 0.02]\n",
        "})\n",
        "\n",
        "# Sample chat messages\n",
        "chat_data = [\n",
        "    \"This stream is amazing!\",\n",
        "    \"Why is it lagging so much?\",\n",
        "    \"The quality is really good today.\",\n",
        "    \"Buffering again... not great.\"\n",
        "]\n",
        "\n",
        "# Run the main loop\n",
        "main_loop(qos_data, chat_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dZa4Rdn3s7_U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZa4Rdn3s7_U",
        "outputId": "ec1a0307-d9ef-4e41-c585-7fb25732168d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Increasing bandwidth and reducing latency.\n",
            "Iteration 1: QoS Prediction: tensor([[0.4193, 0.5807]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 2: QoS Prediction: tensor([[0.5411, 0.4589]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Increasing bandwidth and reducing latency.\n",
            "Iteration 3: QoS Prediction: tensor([[0.4582, 0.5418]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Increasing bandwidth and reducing latency.\n",
            "Iteration 4: QoS Prediction: tensor([[0.4759, 0.5241]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n"
          ]
        }
      ],
      "source": [
        "# High latency and packet loss scenario\n",
        "qos_data = pd.DataFrame({\n",
        "    'latency': [100, 150, 120, 130],\n",
        "    'bitrate': [2000, 1800, 2100, 1900],\n",
        "    'packet_loss': [0.15, 0.20, 0.18, 0.17]\n",
        "})\n",
        "\n",
        "# Chat data indicating user frustration\n",
        "chat_data = [\n",
        "    \"The stream is lagging so much!\",\n",
        "    \"Can't watch, it's buffering every minute.\",\n",
        "    \"What's going on with the quality today?\",\n",
        "    \"This is unwatchable!\"\n",
        "]\n",
        "\n",
        "# Run the main loop\n",
        "main_loop(qos_data, chat_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xWcSk1JFs8t3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWcSk1JFs8t3",
        "outputId": "49608b05-dd97-4ea9-f818-a93f2454da8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stable QoS parameters.\n",
            "Iteration 1: QoS Prediction: tensor([[0.5684, 0.4316]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 2: QoS Prediction: tensor([[0.5667, 0.4333]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 3: QoS Prediction: tensor([[0.6654, 0.3346]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 4: QoS Prediction: tensor([[0.5771, 0.4229]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n"
          ]
        }
      ],
      "source": [
        "# Smooth streaming metrics\n",
        "qos_data = pd.DataFrame({\n",
        "    'latency': [10, 12, 8, 15],\n",
        "    'bitrate': [6000, 6200, 5900, 6100],\n",
        "    'packet_loss': [0.01, 0.02, 0.00, 0.01]\n",
        "})\n",
        "\n",
        "# Chat data showing user satisfaction\n",
        "chat_data = [\n",
        "    \"This stream is crystal clear!\",\n",
        "    \"Loving the quality today!\",\n",
        "    \"Perfect stream, no lag at all.\",\n",
        "    \"Best quality I've seen so far!\"\n",
        "]\n",
        "\n",
        "# Run the main loop\n",
        "main_loop(qos_data, chat_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T2jkg8YDs-RQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2jkg8YDs-RQ",
        "outputId": "28f66bfb-68e6-4520-bbbd-6cbb3bbfc254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stable QoS parameters.\n",
            "Iteration 1: QoS Prediction: tensor([[0.5645, 0.4355]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 2: QoS Prediction: tensor([[0.5892, 0.4108]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 3: QoS Prediction: tensor([[0.5817, 0.4183]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 4: QoS Prediction: tensor([[0.6084, 0.3916]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n"
          ]
        }
      ],
      "source": [
        "# Mixed quality streaming\n",
        "qos_data = pd.DataFrame({\n",
        "    'latency': [25, 45, 20, 60],\n",
        "    'bitrate': [5000, 4800, 5300, 4700],\n",
        "    'packet_loss': [0.05, 0.10, 0.03, 0.12]\n",
        "})\n",
        "\n",
        "# Mixed user sentiment\n",
        "chat_data = [\n",
        "    \"It's fine most of the time, but sometimes it lags.\",\n",
        "    \"A bit choppy but still watchable.\",\n",
        "    \"Smooth at first, but now it's buffering a lot.\",\n",
        "    \"Why is it stuttering so much now?\"\n",
        "]\n",
        "\n",
        "# Run the main loop\n",
        "main_loop(qos_data, chat_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JxV2Jz1ds_-5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxV2Jz1ds_-5",
        "outputId": "803ce8aa-04ff-466d-d860-4e46030333ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stable QoS parameters.\n",
            "Iteration 1: QoS Prediction: tensor([[0.5876, 0.4124]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 2: QoS Prediction: tensor([[0.5876, 0.4124]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 3: QoS Prediction: tensor([[0.5876, 0.4124]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 4: QoS Prediction: tensor([[0.5876, 0.4124]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n"
          ]
        }
      ],
      "source": [
        "# Idle or very low data streams\n",
        "qos_data = pd.DataFrame({\n",
        "    'latency': [5, 5, 5, 5],\n",
        "    'bitrate': [1000, 1000, 1000, 1000],\n",
        "    'packet_loss': [0.00, 0.00, 0.00, 0.00]\n",
        "})\n",
        "\n",
        "# Neutral chat messages\n",
        "chat_data = [\n",
        "    \"Stream is okay.\",\n",
        "    \"Nothing much happening right now.\",\n",
        "    \"Just waiting for the game to start.\",\n",
        "    \"Chat is kinda quiet today.\"\n",
        "]\n",
        "\n",
        "# Run the main loop\n",
        "main_loop(qos_data, chat_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J8avqn_-tEm7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8avqn_-tEm7",
        "outputId": "ad3e8b48-8d48-4290-bcca-bcd9f866e5cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Increasing bandwidth and reducing latency.\n",
            "Iteration 1: QoS Prediction: tensor([[0.4723, 0.5277]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Increasing bandwidth and reducing latency.\n",
            "Iteration 2: QoS Prediction: tensor([[0.4613, 0.5387]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Stable QoS parameters.\n",
            "Iteration 3: QoS Prediction: tensor([[0.5116, 0.4884]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n",
            "Increasing bandwidth and reducing latency.\n",
            "Iteration 4: QoS Prediction: tensor([[0.4889, 0.5111]], grad_fn=<SoftmaxBackward0>), Chat Sentiment: tensor([0])\n"
          ]
        }
      ],
      "source": [
        "# High bitrate for HD or 4K streaming\n",
        "qos_data = pd.DataFrame({\n",
        "    'latency': [10, 8, 12, 9],\n",
        "    'bitrate': [8000, 8200, 7900, 8100],\n",
        "    'packet_loss': [0.02, 0.01, 0.01, 0.00]\n",
        "})\n",
        "\n",
        "# Positive feedback in the chat\n",
        "chat_data = [\n",
        "    \"The stream is so smooth!\",\n",
        "    \"I love the 4K quality, it's amazing.\",\n",
        "    \"Great job with the stream, perfect clarity.\",\n",
        "    \"This is e-sports quality for sure!\"\n",
        "]\n",
        "\n",
        "# Run the main loop\n",
        "main_loop(qos_data, chat_data)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
